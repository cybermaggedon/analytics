
Analytics cluster
=================

Component containers
--------------------

gcr.io/trust-networks/analytics:
  A streaming analytics configuration with the Trust Networks analytics.
  
cassandra:
  Standard Cassandra deployment
  
elasticsearch:
  Standard ElasticSearch deployment
  
cybermaggedon/gaffer:
  Gaffer (graph store).  Deployed, but not currently used.

cybermaggedon/cybermon:
  The cybermon part of the cyberprobe system.  Receives IP packets, and
  outputs events.  Here, we configure to deliver to the streaming analytics
  service.  The cybermon container is safe to scale up, and use behind a
  load balancer, as there is no cross-stream state.
  
kibana:
  Standard Kibana deployment.
  
cybermaggedon/sparql-cassandra:
  A SPARQL endpoint for the data we store in Cassandra.
  
gcr.io/trust-networks/web:
  Web front-end, hosts static code, and also proxies out to the other
  web-based services.

gcr.io/trust-networks/ca:
  Certificate Authority service for Trust Networks, containing the
  Trust Networks private key.  The CA container offers no external services.
  The only way to interact with it is to connect using the Google Cloud API
  for which you need a private key from the cloud project.

gcr.io/trust-networks/vpn-ca:
  Certificate Authority service for the Trust Networks VPN, containing
  the CA private key.  The CA container offers no external services.
  The only way to interact with it is to connect using the Google Cloud API
  for which you need a private key from the cloud project.

gcr.io/trust-networks/vpn-server:
  Contains an OpenVPN server on UDP port 1194, and cyberprobe configured to
  grab the packets, and deliver to cybermon.  This container is good to use
  behind a load-balancer.  The load-balancer will distribute on IP address,
  so sharing the load, and there is no cross-VPN state.

  No configuration is needed for new VPN clients.  When a new client connects
  an IP address is allocated, and the CN parameter in the certificate defines
  client name.  The client name and IP address are written to ipp.txt, and
  cyberprobe-sync spots the config change and updates cyberprobe.cfg, which
  makes sure the client name is tagged onto the stream.

Container masters
-----------------

These containers are official Docker containers:
  cassandra, elasticsearch, kibana

These containers are maintained by cybermaggedon:
  cybermaggedon/gaffer, cybermaggedon/cybermon, cybermaggedon/sparql-cassandra

These containers are managed here:
  gcr.io/trust-networks/analytics
  gcr.io/trust-networks/web
  gcr.io/trust-networks/ca

vpn-ca and vpn-server are managed at github.com/trustnetworks/vpn-service:
  gcr.io/trust-networks/vpn-ca
  gcr.io/trust-networks/vpn-server
  
Maintaining the containers
--------------------------

If you are pushing a new container, update the version.  Otherwise, Google
will cache the old version.

To change a container version, change the appropriate Makefile, and change
the image entry in analytics-deployments.yaml

Note for editing YAML files: No tabs!  All indent must be spaces!

The CA certificate
------------------

If you re-created and push new CA certificates, you will invalidate
everyone's existing certificate.  Probably best to leave certificates alone.

Certificates
------------

The private/public keys for web server and CA only need to be created once,
and then again when they expire.

So, best to leave the certificates alone!

Unless you need to (expiry or a security problem) in which case, you'll want
to throw the old key away and create a new one, but APPEND the new CA cert
to the certificate list, so as to do a soft changeover.

  cd certs
  make clean
  make
  make delete
  make upload

CA container
------------

  cd ca
  make
  make push

Web container
-------------

  cd web
  make
  make push

Analytics cluster
-----------------

  cd analyics
  make
  make push
  
Deploying the cluster
---------------------

gcloud init

gcloud config set compute/zone us-east1-b

# 2 node cluster, n1-standard-4
a=https://www.googleapis.com/auth
gcloud container --project "trust-networks" clusters create "analytics" \
  --zone "us-east1-b" --machine-type "n1-standard-4" \
  --scopes "${a}/compute","${a}/devstorage.read_only","${a}/logging.write","${a}/monitoring","${a}/servicecontrol","${a}/service.management.readonly" \
  --num-nodes "2" --network "default" --enable-cloud-logging \
  --enable-cloud-monitoring
  
# 25 GB disk for Cassandra
gcloud compute --project "trust-networks" disks create "cassandra-0000" \
  --size "25" --zone "us-east1-b" --type "pd-ssd"

# 50 GB disk for ES
gcloud compute --project "trust-networks" disks create "elasticsearch-0000" \
  --size "50" --zone "us-east1-b" --type "pd-ssd"

# 1 GB disk for CA
gcloud compute --project "trust-networks" disks create "ca-0000" \
  --size "1" --zone "us-east1-b" --type "pd-standard"

# 1 GB disk for VPN CA
gcloud compute --project "trust-networks" disks create "vpn-ca-data-0000" \
  --size "1" --zone "us-east1-b" --type "pd-standard"

# 50 GB disk for Gaffer Hadoop
gcloud compute --project "trust-networks" disks create "hadoop-0000" \
  --size "50" --zone "us-east1-b" --type "pd-ssd"

# 10 GB disk for Gaffer Zookeeper
gcloud compute --project "trust-networks" disks create "zookeeper-1" \
  --size "10" --zone "us-east1-b" --type "pd-ssd"

# 10 GB disk for Gaffer Zookeeper
gcloud compute --project "trust-networks" disks create "zookeeper-2" \
  --size "10" --zone "us-east1-b" --type "pd-ssd"

# 10 GB disk for Gaffer Zookeeper
gcloud compute --project "trust-networks" disks create "zookeeper-3" \
  --size "10" --zone "us-east1-b" --type "pd-ssd"

gcloud container clusters get-credentials analytics \
  --zone us-east1-b --project trust-networks

# For this step, you need to have an appropriate private.json key file
# from the Google cloud console.  The key file is for a service with the
# appropriate permissions.
kubectl create secret generic keys --from-file=private.json

kubectl create -f analytics-services.yaml

kubectl create -f analytics-deployments.yaml

Once the services are deployed, (check kubectl get services) they will
get extern IP addresses.  You need to manually configure the trustnetworks
domain with their IP addresses:

  cybermon -> cybermon.ops.trustnetworks.com
  publisher -> publisher.ops.trustnetworks.com
  vpn -> vpn.ops.trustnetworks.com
  web -> analytics.ops.trustnetworks.com

Upgrading the cluster
---------------------

Easiest way, you can just delete the particular deployment and re-create e.g.

  kubectl delete deployment analytics
  kubectl create -f analytics-deployments.yaml

The create command ignores deployments which already exist.

More elaborate options e.g. rolling upgrades are possible, but you need to
read the documentation.

Deleting the cluster
--------------------

# Tidy up

kubectl delete -f analytics-services.yaml
kubectl delete -f analytics-deployments.yaml

gcloud compute --project "trust-networks" disks delete --zone "us-east1-b" \
       gaffer-hadoop-0000 gaffer-accumulo-0000 gaffer-zookeeper-0000 \
       elasticsearch-0000 cassandra-0000 ca-0000 vpn-ca-data-0000

